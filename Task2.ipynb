{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81afeadc",
   "metadata": {},
   "source": [
    "## Task 2 - CLIP Fine-Tuning on the Visual Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eb2e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title GPU / Python / Torch sanity\n",
    "import os, sys, subprocess, json, platform, torch\n",
    "print(\"Python :\", sys.version)\n",
    "print(\"CUDA   :\", torch.version.cuda)\n",
    "print(\"Torch  :\", torch.__version__)\n",
    "print(\"Device :\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "!nvidia-smi || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab32194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some imports\n",
    "import os, time, math, random\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" # stable ordering\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel, logging\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from torchinfo import summary\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Ensure reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some settings\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-large-patch14\" # pre-trained CLIP model (ViT-L/14)\n",
    "BATCH_SIZE = 32  # chosen for RTX 3090 (24GB); adjust if you hit OOM\n",
    "gradient_accumulation_steps = 1 # adjust based on your GPU memory\n",
    "# For Linear Probe & LoRA\n",
    "NUM_EPOCHS = 10  # increase to >3 for better curves once everything runs\n",
    "SEED = 42\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(\"device_count:\", torch.cuda.device_count())\n",
    "print(\"device 0:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "# Default LRs\n",
    "LR_HEAD_LINEAR = 1e-3   # linear probe head lr\n",
    "LR_HEAD_LORA   = 1e-3   # LoRA run head lr\n",
    "LR_LORA        = 1e-4   # LoRA adapter lr\n",
    "WEIGHT_DECAY   = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53344f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP settings\n",
    "# --- Load CLIP Processor ---\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "# --- Define a transform to process images for CLIP ---\n",
    "class CLIPTransform:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # The processor expects a PIL image or list of images\n",
    "        # It returns a dict, we extract 'pixel_values'\n",
    "        # .squeeze(0) removes the batch dimension the processor adds\n",
    "        return self.processor(images=image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "clip_transform = CLIPTransform(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365bb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset related imports\n",
    "from torchvision.datasets import Flowers102 \n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- Flowers102 ---\n",
    "# prepare Flowers102 dataset\n",
    "flowers102_test_dts = Flowers102(root=\".\", split=\"test\", transform=clip_transform) # evaluation on this set\n",
    "flowers102_train_dts = Flowers102(root=\".\", split=\"train\", transform=clip_transform)\n",
    "flowers102_val_dts = Flowers102(root=\".\", split=\"val\", transform=clip_transform)\n",
    "\n",
    "print(f\"Total training samples (orig train): {len(flowers102_train_dts)}\")\n",
    "print(f\"Total validation samples (orig val): {len(flowers102_val_dts)}\")\n",
    "print(f\"Total test samples: {len(flowers102_test_dts)}\") # should be 6149\n",
    "\n",
    "# prepare class names for Flowers102\n",
    "with open(\"cat_to_name.json\", \"r\") as f:\n",
    "    cat_to_name = json.load(f)\n",
    "\n",
    "flowers102_class_names = [cat_to_name[str(i + 1)] for i in range(102)]\n",
    "\n",
    "# --- CUB-200-2011 ---\n",
    "birds_200 = load_dataset(\"bentrevett/caltech-ucsd-birds-200-2011\")\n",
    "split = birds_200[\"train\"].train_test_split(test_size=0.1, seed=42, shuffle=True)\n",
    "\n",
    "cub_bird_train_dts = split[\"train\"]\n",
    "cub_bird_val_dts = split[\"test\"]\n",
    "cub_bird_test_dts = birds_200[\"test\"]\n",
    "\n",
    "print(f\"Total training samples: {len(cub_bird_train_dts)}\")\n",
    "print(f\"Total validation samples: {len(cub_bird_val_dts)}\")\n",
    "print(f\"Total test samples: {len(cub_bird_test_dts)}\") # should be 5794\n",
    "\n",
    "# prepare class names for CUB-200-2011\n",
    "cub_class_names_raw = birds_200[\"train\"].features[\"label\"].names\n",
    "cub_class_names = [name.split('.')[-1].replace('_', ' ') for name in cub_class_names_raw]\n",
    "\n",
    "# Wrap HF dataset to return (pixel_values, label) tensors compatible with default collate\n",
    "class HFCUBWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_ds):\n",
    "        self.hf_ds = hf_ds\n",
    "    def __len__(self):\n",
    "        return len(self.hf_ds)\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.hf_ds[idx]\n",
    "        pv = clip_transform(ex[\"image\"])  # Tensor [3, 224, 224]\n",
    "        lbl = int(ex[\"label\"])            # int\n",
    "        return pv, lbl\n",
    "    \n",
    "cub_bird_train_dts = HFCUBWrapper(cub_bird_train_dts)\n",
    "cub_bird_val_dts = HFCUBWrapper(cub_bird_val_dts)\n",
    "cub_bird_test_dts = HFCUBWrapper(cub_bird_test_dts)\n",
    "\n",
    "# === Create DataLoaders (only test for now, train/val in next cell) ===\n",
    "flowers102_test_loader = DataLoader(flowers102_test_dts, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "cub200_test_loader = DataLoader(cub_bird_test_dts, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Train/Val splits and DataLoaders ===\n",
    "# Flowers: merge original train+val (2040) and split into 1836 train / 204 val\n",
    "from torch.utils.data import ConcatDataset\n",
    "flowers_trainval = ConcatDataset([flowers102_train_dts, flowers102_val_dts])\n",
    "assert len(flowers_trainval) == (len(flowers102_train_dts) + len(flowers102_val_dts)) == 2040\n",
    "train_len, val_len = 1836, 204\n",
    "g = torch.Generator().manual_seed(SEED)\n",
    "flowers_train_dts, flowers_val_dts_new = random_split(flowers_trainval, [train_len, val_len], generator=g)\n",
    "flowers_train_loader = DataLoader(flowers_train_dts, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "flowers_val_loader   = DataLoader(flowers_val_dts_new, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "flowers_test_loader  = flowers102_test_loader\n",
    "print(f\"Flowers -> train: {len(flowers_train_dts)}, val: {len(flowers_val_dts_new)}, test: {len(flowers102_test_dts)}\")\n",
    "\n",
    "# CUB: we already did a 90/10 split to get 5394/600\n",
    "cub_train_loader = DataLoader(cub_bird_train_dts, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "cub_val_loader   = DataLoader(cub_bird_val_dts, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "cub_test_loader  = cub200_test_loader\n",
    "print(f\"CUB-200 -> train: {len(cub_bird_train_dts)}, val: {len(cub_bird_val_dts)}, test: {len(cub_bird_test_dts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training utilities ===\n",
    "def accuracy_from_logits(logits: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    return (preds == targets).float().mean().item()\n",
    "\n",
    "# Safe image features that work with/without LoRA wrapping\n",
    "def get_image_features_safe(model, pixel_values: torch.Tensor, require_backbone_grad: bool):\n",
    "    def compute():\n",
    "        # First try the official helper\n",
    "        try:\n",
    "            return model.get_image_features(pixel_values=pixel_values)\n",
    "        except TypeError:\n",
    "            # Manual: vision forward -> pooled -> visual projection -> l2 norm\n",
    "            vision_out = model.vision_model(pixel_values=pixel_values, return_dict=True)\n",
    "            pooled = getattr(vision_out, \"pooler_output\", None)\n",
    "            if pooled is None:\n",
    "                pooled = vision_out[1]\n",
    "            feats = model.visual_projection(pooled)\n",
    "            feats = feats / feats.norm(dim=-1, keepdim=True)\n",
    "            return feats\n",
    "\n",
    "    if require_backbone_grad:\n",
    "        return compute()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return compute()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_epoch(model, head, data_loader, device, require_backbone_grad: bool):\n",
    "    model.eval()\n",
    "    head.eval()\n",
    "    total_loss, total_acc, total_n = 0.0, 0.0, 0\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    for pixel_values, labels in data_loader:\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        feats = get_image_features_safe(model, pixel_values, require_backbone_grad=False)\n",
    "        logits = head(feats)\n",
    "        loss = ce(logits, labels)\n",
    "        bs = labels.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += (logits.argmax(-1) == labels).sum().item()\n",
    "        total_n    += bs\n",
    "    return total_loss / max(1,total_n), total_acc / max(1,total_n)\n",
    "\n",
    "def train_epoch(model, head, data_loader, device, optimizer, grad_accum_steps: int, require_backbone_grad: bool):\n",
    "    model.train() if require_backbone_grad else model.eval()\n",
    "    head.train()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    total_loss, total_acc, total_n = 0.0, 0.0, 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for step, (pixel_values, labels) in enumerate(tqdm(data_loader, leave=False)):\n",
    "        pixel_values = pixel_values.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        feats = get_image_features_safe(model, pixel_values, require_backbone_grad=require_backbone_grad)\n",
    "        logits = head(feats)\n",
    "        loss = ce(logits, labels) / grad_accum_steps\n",
    "        loss.backward()\n",
    "        if (step + 1) % grad_accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        bs = labels.size(0)\n",
    "        total_loss += loss.item() * grad_accum_steps * bs\n",
    "        total_acc  += (logits.argmax(-1) == labels).sum().item()\n",
    "        total_n    += bs\n",
    "    return total_loss / max(1,total_n), total_acc / max(1,total_n)\n",
    "\n",
    "def plot_curves(history, title_prefix=\"\", save_path: str | None = None):\n",
    "    train_loss, val_loss = history['train_loss'], history['val_loss']\n",
    "    train_acc,  val_acc  = history['train_acc'],  history['val_acc']\n",
    "    epochs = range(1, len(train_loss)+1)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_loss, label='train')\n",
    "    plt.plot(epochs, val_loss, label='val')\n",
    "    plt.xlabel('epoch'); plt.ylabel('loss'); plt.title(f'{title_prefix} loss'); plt.legend()\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, train_acc, label='train')\n",
    "    plt.plot(epochs, val_acc, label='val')\n",
    "    plt.xlabel('epoch'); plt.ylabel('accuracy'); plt.title(f'{title_prefix} accuracy'); plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Method: Linear Probing ---\")\n",
    "\n",
    "# 1) Load full CLIP (we'll only use the vision tower)\n",
    "clip_linear = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "vision_model = clip_linear.vision_model\n",
    "visual_projection = clip_linear.visual_projection\n",
    "\n",
    "# 2) Freeze backbone and projection (no grads in vision encoder)\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in visual_projection.parameters():\n",
    "    p.requires_grad = False\n",
    "clip_linear.eval()  # ensure backbone runs in eval for deterministic features\n",
    "\n",
    "# Helper to run one dataset\n",
    "\n",
    "def run_linear_probe_one(name, num_classes, loaders, save_prefix):\n",
    "    train_loader, val_loader, test_loader = loaders\n",
    "    # Feature dim from projected image features\n",
    "    in_dim = visual_projection.out_features if hasattr(visual_projection, 'out_features') else clip_linear.config.projection_dim\n",
    "    head = nn.Linear(in_dim, num_classes).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(head.parameters(), lr=LR_HEAD_LINEAR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        tr_loss, tr_acc = train_epoch(clip_linear, head, train_loader, DEVICE, optimizer, gradient_accumulation_steps, require_backbone_grad=False)\n",
    "        val_loss, val_acc = evaluate_epoch(clip_linear, head, val_loader, DEVICE, require_backbone_grad=False)\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        print(f\"[{name}] Epoch {epoch+1}/{NUM_EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc*100:.2f}% | val loss {val_loss:.4f} acc {val_acc*100:.2f}%\")\n",
    "\n",
    "    # Save curves and head\n",
    "    save_history_png(history, title_prefix=f\"LinearProbe {name}\", filename=f\"linear_{save_prefix}_curves.png\")\n",
    "    torch.save(head.state_dict(), CKPT_DIR / f\"linear_{save_prefix}_head.pt\")\n",
    "    print(f\"Saved head: {CKPT_DIR / f'linear_{save_prefix}_head.pt'}\")\n",
    "\n",
    "    test_loss, test_acc = evaluate_epoch(clip_linear, head, test_loader, DEVICE, require_backbone_grad=False)\n",
    "    append_metrics_csv([{\"method\":\"Linear\",\"dataset\":name,\"epochs\":NUM_EPOCHS,\"test_acc\":test_acc,\"val_acc\":history['val_acc'][-1]}])\n",
    "    print(f\"[{name}] Test: loss {test_loss:.4f} | acc {test_acc*100:.2f}%\")\n",
    "    return head, history, {\"test_loss\":test_loss, \"test_acc\":test_acc}\n",
    "\n",
    "# Flowers-102 (102 classes)\n",
    "flowers_linear_head, flowers_linear_hist, flowers_linear_test = run_linear_probe_one(\n",
    "    name=\"Flowers102\",\n",
    "    num_classes=102,\n",
    "    loaders=(flowers_train_loader, flowers_val_loader, flowers_test_loader),\n",
    "    save_prefix=\"flowers\"\n",
    ")\n",
    "\n",
    "# CUB-200-2011 (200 classes)\n",
    "cub_linear_head, cub_linear_hist, cub_linear_test = run_linear_probe_one(\n",
    "    name=\"CUB-200\",\n",
    "    num_classes=200,\n",
    "    loaders=(cub_train_loader, cub_val_loader, cub_test_loader),\n",
    "    save_prefix=\"cub\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f7832",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Starting Method: LoRA Fine-Tuning ---\")\n",
    "\n",
    "# 1) Load CLIP and inject LoRA into the full CLIP model (targets still match vision modules)\n",
    "clip_lora = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "visual_projection = clip_lora.visual_projection\n",
    "\n",
    "# 2) LoRA config (target Q/V projections in ViT attention blocks)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # matches both text/vision; only vision path gets gradients\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "\n",
    "# 3) Wrap with PEFT at the CLIPModel level\n",
    "clip_lora = get_peft_model(clip_lora, lora_config)\n",
    "print(\"LoRA Model - Trainable Parameters:\")\n",
    "clip_lora.print_trainable_parameters()\n",
    "\n",
    "# 4) Freeze non-LoRA weights and the visual projection\n",
    "for n, p in clip_lora.named_parameters():\n",
    "    if \"lora_\" in n:\n",
    "        p.requires_grad = True\n",
    "    else:\n",
    "        p.requires_grad = False\n",
    "for p in visual_projection.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Helper to run one dataset with LoRA\n",
    "def run_lora_one(name, num_classes, loaders, save_prefix):\n",
    "    train_loader, val_loader, test_loader = loaders\n",
    "    in_dim = visual_projection.out_features if hasattr(visual_projection, 'out_features') else clip_lora.config.projection_dim\n",
    "    head = nn.Linear(in_dim, num_classes).to(DEVICE)\n",
    "\n",
    "    # Two parameter groups: LoRA adapters and head\n",
    "    lora_params = [p for p in clip_lora.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {\"params\": lora_params, \"lr\": LR_LORA},\n",
    "        {\"params\": head.parameters(), \"lr\": LR_HEAD_LORA},\n",
    "    ], weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        tr_loss, tr_acc = train_epoch(clip_lora, head, train_loader, DEVICE, optimizer, gradient_accumulation_steps, require_backbone_grad=True)\n",
    "        val_loss, val_acc = evaluate_epoch(clip_lora, head, val_loader, DEVICE, require_backbone_grad=True)\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        print(f\"[LoRA {name}] Epoch {epoch+1}/{NUM_EPOCHS} | train loss {tr_loss:.4f} acc {tr_acc*100:.2f}% | val loss {val_loss:.4f} acc {val_acc*100:.2f}%\")\n",
    "\n",
    "    # Save curves and adapters/head\n",
    "    save_history_png(history, title_prefix=f\"LoRA {name}\", filename=f\"lora_{save_prefix}_curves.png\")\n",
    "    torch.save(head.state_dict(), CKPT_DIR / f\"lora_{save_prefix}_head.pt\")\n",
    "    torch.save(clip_lora.state_dict(), CKPT_DIR / f\"lora_{save_prefix}_clip.pt\")\n",
    "    print(f\"Saved head: {CKPT_DIR / f'lora_{save_prefix}_head.pt'}\")\n",
    "    print(f\"Saved LoRA model: {CKPT_DIR / f'lora_{save_prefix}_clip.pt'}\")\n",
    "\n",
    "    test_loss, test_acc = evaluate_epoch(clip_lora, head, test_loader, DEVICE, require_backbone_grad=True)\n",
    "    append_metrics_csv([{\"method\":\"LoRA\",\"dataset\":name,\"epochs\":NUM_EPOCHS,\"test_acc\":test_acc,\"val_acc\":history['val_acc'][-1]}])\n",
    "    print(f\"[LoRA {name}] Test: loss {test_loss:.4f} | acc {test_acc*100:.2f}%\")\n",
    "    return head, history, {\"test_loss\":test_loss, \"test_acc\":test_acc}, clip_lora\n",
    "\n",
    "# Flowers-102 (102 classes)\n",
    "flowers_lora_head, flowers_lora_hist, flowers_lora_test, flowers_lora_model = run_lora_one(\n",
    "    name=\"Flowers102\",\n",
    "    num_classes=102,\n",
    "    loaders=(flowers_train_loader, flowers_val_loader, flowers_test_loader),\n",
    "    save_prefix=\"flowers\"\n",
    ")\n",
    "\n",
    "# CUB-200-2011 (200 classes)\n",
    "cub_lora_head, cub_lora_hist, cub_lora_test, cub_lora_model = run_lora_one(\n",
    "    name=\"CUB-200\",\n",
    "    num_classes=200,\n",
    "    loaders=(cub_train_loader, cub_val_loader, cub_test_loader),\n",
    "    save_prefix=\"cub\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Results saving helpers ===\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "RESULTS_DIR = Path(\"results/task2\")\n",
    "CKPT_DIR = Path(\"checkpoints/task2\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_history_png(history, title_prefix, filename):\n",
    "    out = RESULTS_DIR / filename\n",
    "    plot_curves(history, title_prefix=title_prefix, save_path=str(out))\n",
    "    print(f\"Saved curve: {out}\")\n",
    "\n",
    "def append_metrics_csv(rows, filename=\"metrics.csv\"):\n",
    "    out = RESULTS_DIR / filename\n",
    "    df = pd.DataFrame(rows)\n",
    "    if out.exists():\n",
    "        old = pd.read_csv(out)\n",
    "        df = pd.concat([old, df], ignore_index=True)\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"Saved metrics: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NTU images: inference on 4 trained models ===\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_image_tensor(path):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    pv = clip_transform(img)  # [3,224,224]\n",
    "    return pv.unsqueeze(0).to(DEVICE), img\n",
    "\n",
    "def topk_from_logits(logits, class_names, k=5):\n",
    "    probs = F.softmax(logits, dim=-1).squeeze(0)\n",
    "    topk = torch.topk(probs, k)\n",
    "    idxs = topk.indices.tolist()\n",
    "    vals = topk.values.tolist()\n",
    "    return [(class_names[i], float(v)) for i, v in zip(idxs, vals)]\n",
    "\n",
    "def predict_one(model, head, image_path, class_names, title_prefix, save_name):\n",
    "    pixel_values, img = load_image_tensor(image_path)\n",
    "    feats = get_image_features_safe(model, pixel_values, require_backbone_grad=False)\n",
    "    logits = head(feats)\n",
    "    topk = topk_from_logits(logits, class_names, k=5)\n",
    "    # Plot bar\n",
    "    labels = [l for l,_ in topk][::-1]\n",
    "    vals   = [v for _,v in topk][::-1]\n",
    "    plt.figure(figsize=(6,3))\n",
    "    sns.barplot(x=vals, y=labels, palette=\"mako\")\n",
    "    plt.xlim(0,1)\n",
    "    for i,v in enumerate(vals):\n",
    "        plt.text(v+0.01, i, f\"{v*100:.1f}%\", va='center')\n",
    "    plt.title(f\"{title_prefix}\")\n",
    "    out = RESULTS_DIR / save_name\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {out}\")\n",
    "    return topk\n",
    "\n",
    "bird_path = \"img/bird_ntu.jpg\"\n",
    "flower_path = \"img/flower_ntu.jpg\"\n",
    "\n",
    "# Linear probe models use the frozen clip_linear for features\n",
    "if 'clip_linear' in globals() and 'flowers_linear_head' in globals() and 'cub_linear_head' in globals():\n",
    "    print(\"-- Linear probe: NTU images --\")\n",
    "    predict_one(clip_linear, flowers_linear_head, flower_path, flowers102_class_names, \"Linear Flowers102 - flower_ntu\", \"ntu_linear_flowers_flower.png\")\n",
    "    predict_one(clip_linear, cub_linear_head,     bird_path,   cub_class_names,        \"Linear CUB-200 - bird_ntu\",     \"ntu_linear_cub_bird.png\")\n",
    "else:\n",
    "    print(\"Linear probe heads not found. Run the Linear Probing cell first.\")\n",
    "\n",
    "# LoRA models use their respective clip_lora_model instances returned\n",
    "if 'flowers_lora_model' in globals() and 'flowers_lora_head' in globals():\n",
    "    print(\"-- LoRA Flowers: NTU images --\")\n",
    "    predict_one(flowers_lora_model, flowers_lora_head, flower_path, flowers102_class_names, \"LoRA Flowers102 - flower_ntu\", \"ntu_lora_flowers_flower.png\")\n",
    "else:\n",
    "    print(\"LoRA Flowers model not found. Run the LoRA cell.\")\n",
    "\n",
    "if 'cub_lora_model' in globals() and 'cub_lora_head' in globals():\n",
    "    print(\"-- LoRA CUB: NTU images --\")\n",
    "    predict_one(cub_lora_model, cub_lora_head, bird_path,   cub_class_names, \"LoRA CUB-200 - bird_ntu\",   \"ntu_lora_cub_bird.png\")\n",
    "else:\n",
    "    print(\"LoRA CUB model not found. Run the LoRA cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Optional: load checkpoints and run NTU predictions without retraining ===\n",
    "def load_linear_head(save_prefix, in_dim, num_classes):\n",
    "    head = nn.Linear(in_dim, num_classes).to(DEVICE)\n",
    "    sd = torch.load(CKPT_DIR / f\"linear_{save_prefix}_head.pt\", map_location=DEVICE)\n",
    "    head.load_state_dict(sd)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "def load_lora_model(save_prefix):\n",
    "    # Reload base CLIP and then load raw state dict (saved above)\n",
    "    mdl = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "    mdl = get_peft_model(mdl, lora_config)\n",
    "    sd = torch.load(CKPT_DIR / f\"lora_{save_prefix}_clip.pt\", map_location=DEVICE)\n",
    "    mdl.load_state_dict(sd)\n",
    "    return mdl\n",
    "\n",
    "def try_ntu_from_checkpoints():\n",
    "    try:\n",
    "        # Linear\n",
    "        base = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "        base.eval()\n",
    "        in_dim = base.visual_projection.out_features if hasattr(base.visual_projection,'out_features') else base.config.projection_dim\n",
    "        fl_head = load_linear_head('flowers', in_dim, 102)\n",
    "        cb_head = load_linear_head('cub', in_dim, 200)\n",
    "        print(\"-- Linear from checkpoints --\")\n",
    "        predict_one(base, fl_head, 'img/flower_ntu.jpg', flowers102_class_names, 'Linear Flowers102 - flower_ntu (ckpt)', 'ntu_linear_ckpt_flowers_flower.png')\n",
    "        predict_one(base, fl_head, 'img/bird_ntu.jpg',   flowers102_class_names, 'Linear Flowers102 - bird_ntu (ckpt)',   'ntu_linear_ckpt_flowers_bird.png')\n",
    "        predict_one(base, cb_head, 'img/flower_ntu.jpg', cub_class_names,        'Linear CUB-200 - flower_ntu (ckpt)',   'ntu_linear_ckpt_cub_flower.png')\n",
    "        predict_one(base, cb_head, 'img/bird_ntu.jpg',   cub_class_names,        'Linear CUB-200 - bird_ntu (ckpt)',     'ntu_linear_ckpt_cub_bird.png')\n",
    "    except Exception as e:\n",
    "        print(\"Linear checkpoints not available:\", e)\n",
    "    try:\n",
    "        print(\"-- LoRA from checkpoints --\")\n",
    "        fl_m = load_lora_model('flowers')\n",
    "        fl_h = torch.load(CKPT_DIR / 'lora_flowers_head.pt', map_location=DEVICE)\n",
    "        head_fl = nn.Linear(fl_m.visual_projection.out_features if hasattr(fl_m.visual_projection,'out_features') else fl_m.config.projection_dim, 102).to(DEVICE)\n",
    "        head_fl.load_state_dict(fl_h); head_fl.eval()\n",
    "        predict_one(fl_m, head_fl, 'img/flower_ntu.jpg', flowers102_class_names, 'LoRA Flowers102 - flower_ntu (ckpt)', 'ntu_lora_ckpt_flowers_flower.png')\n",
    "        predict_one(fl_m, head_fl, 'img/bird_ntu.jpg',   flowers102_class_names, 'LoRA Flowers102 - bird_ntu (ckpt)',   'ntu_lora_ckpt_flowers_bird.png')\n",
    "    except Exception as e:\n",
    "        print(\"LoRA Flowers checkpoints not available:\", e)\n",
    "    try:\n",
    "        cb_m = load_lora_model('cub')\n",
    "        cb_h = torch.load(CKPT_DIR / 'lora_cub_head.pt', map_location=DEVICE)\n",
    "        head_cb = nn.Linear(cb_m.visual_projection.out_features if hasattr(cb_m.visual_projection,'out_features') else cb_m.config.projection_dim, 200).to(DEVICE)\n",
    "        head_cb.load_state_dict(cb_h); head_cb.eval()\n",
    "        predict_one(cb_m, head_cb, 'img/flower_ntu.jpg', cub_class_names, 'LoRA CUB-200 - flower_ntu (ckpt)', 'ntu_lora_ckpt_cub_flower.png')\n",
    "        predict_one(cb_m, head_cb, 'img/bird_ntu.jpg',   cub_class_names, 'LoRA CUB-200 - bird_ntu (ckpt)',   'ntu_lora_ckpt_cub_bird.png')\n",
    "    except Exception as e:\n",
    "        print(\"LoRA CUB checkpoints not available:\", e)\n",
    "\n",
    "# Uncomment to run if you've trained and saved checkpoints previously\n",
    "# try_ntu_from_checkpoints()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
